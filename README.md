
# ğŸ“˜ Context-Aware Learning Assistant (RAG-Based QA System)

## ğŸ” Project Overview

The **Context-Aware Learning Assistant** is an AI-powered academic question-answering system built using **Retrieval-Augmented Generation (RAG)**.
It answers computer science questions **strictly based on provided academic documents** such as PDFs, ensuring **zero hallucination**, high reliability, and complete **source transparency**.

This project is designed for **exam-oriented learning**, supporting **2-mark, 5-mark, and 13-mark answers**, with confidence scoring and accessibility features like **text-to-speech** and **voice input**.

---

## ğŸš€ Key Features

* ğŸ“š **Document-Based Question Answering**

  * Answers are generated **only from retrieved document context**
  * No use of LLMâ€™s own knowledge

* ğŸ§  **Retrieval-Augmented Generation (RAG)**

  * FAISS vector search for accurate document retrieval
  * Semantic similarity-based chunk selection

* ğŸ“Š **Answer Confidence Scoring**

  * Confidence calculated using retrieval relevance
  * Transparent reliability indicator for each answer

* ğŸ” **Source Transparency**

  * View exact document chunks used to generate answers
  * Relevance score shown for every source

* ğŸ—£ **Text-to-Speech (TTS)**

  * Clean voice output (Markdown removed before reading)
  * Start / Stop reading controls

* ğŸ¤ **Voice Input**

  * Ask questions using speech recognition

* ğŸŒ— **Dark / Light Mode**

  * Modern, responsive UI with accessibility support

* ğŸ“„ **Export & Copy**

  * Export answers as `.txt`
  * One-click copy to clipboard

---

## ğŸ—ï¸ System Architecture

```
User Query
   â†“
React Frontend
   â†“
FastAPI Backend
   â†“
FAISS Vector Search
   â†“
Relevant Document Chunks
   â†“
LLM (Context-Only Prompting)
   â†“
Final Answer + Confidence + Sources
```

---

## ğŸ§° Tech Stack

### Frontend

* âš›ï¸ React.js
* ğŸ¨ Tailwind CSS
* ğŸ§© Lucide Icons
* ğŸ™ Web Speech API (Voice Input & TTS)

### Backend

* ğŸ Python
* âš¡ FastAPI
* ğŸ” FAISS (Vector Database)
* ğŸ¤– Gemini / LLM API
* ğŸ“„ PDF-based document ingestion

---

## ğŸ“‚ Project Structure

```
context/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ query_engine.py
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ rag_retriever.py
â”‚   â”‚   â”œâ”€â”€ faiss_index.pkl
â”‚   â”‚   â””â”€â”€ chunks.pkl
â”‚   â”œâ”€â”€ config.py
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ LearningAssistant.jsx
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ pdfs/
â”‚
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Backend Setup

```bash
cd backend
pip install -r requirements.txt
uvicorn backend.main:app --reload
```

Ensure `config.py` contains your LLM API key:

```python
GEMINI_API_KEY = "your_api_key_here"
```

---

### 2ï¸âƒ£ Frontend Setup

```bash
cd frontend
npm install
npm start
```

Frontend runs at:

```
http://localhost:3000
```

Backend runs at:

```
http://127.0.0.1:8000
```

---

## ğŸ§ª Example Use Case

**Question:**

> Explain deadlock in Operating System for 13 marks

**System Output:**

* âœ” Structured academic answer
* âœ” Confidence score (e.g., 90%)
* âœ” Source documents with relevance %
* âœ” Voice reading available

---

## ğŸ“ Academic Relevance

This project is ideal for:

* Final-year engineering projects
* AI / Data Science portfolios
* RAG system demonstrations
* Exam-oriented learning platforms

---

## ğŸ” Hallucination Control

> â— The model is **strictly instructed to answer ONLY from retrieved context**.
> If the context is insufficient, the system explicitly states that no answer is available.

---

## ğŸŒŸ Future Enhancements

* Multi-language support
* PDF upload via UI
* User authentication
* Answer highlighting inside PDFs
* Cloud deployment

---

## ğŸ‘¨â€ğŸ’» Author

**Bala Murugan L**
B.Tech â€“ Artificial Intelligence & Data Science
AI Enthusiast | RAG Systems | Full-Stack AI Projects

---

## ğŸ“œ License

This project is for **educational and academic use**.

---



